## Web Scraping
*Web scraping* is a term for using a program to download and process content from the web. For example, Google runs many web scraping programs to index web pages for its search engine. Let's learn about the following modules, which make it easy to scrape web pages in Python:
- **`webbrowser`** - Comes with Python and opens a browser to a specific page
- **`requests`** - Downloads files and web pages from the internet
- **Beautiful Soup (`bs4`)** - Parses HTML, the format that web pages are written in, to extract the information you want
- **Selenium** - Launches and controls a web browser, such as by filling in forms and simulating mouse clicks
- **Playwright** - Launches and controls a web browser; newer than Selenium and has some additional features.

### HTTP and HTTPS:
When you visit a website, its web address, such as *https://autbor.com/example3.html*, is known as a *uniform resource locator (URL)*. The *HTTPS* in the URL stands for *Hypertext Transfer Protocol Secure*, the protocol that your web browser uses to access websites.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;More precisely, HTTPS is an encrypted version of HTTP, so it protects your privacy while you use the internet. If you were using HTTP, identity thieves, national intelligence agencies, and your internet service provider could view the content of the web pages you visited, including any passwords and credit card information you submit. Using a virtual private network (VPN) could keep your internet service provider from viewing your internet traffic; however, now the VPN provider would be able to view your traffic. An unscrupulous VPN provider could then sell information about what websites you visit to data brokers.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;By contrast, any web page content you view with HTTPS will be encrypted and hidden. Websites used to use HTTPS only for pages that sent passwords and credit card numbers, but nowadays, most websites encrypt all 
traffic. Keep in mind, though, that the identity of the website you visit can still be known; no one will be able to see exactly what you download from CatPhotos.com, but they will see that you were connecting to the CatPhotos.com website and can figure out that you were probably looking at photos of cats. The Tor Browser, which uses the Tor anonymization network, can provide true anonymous browsing.
#### `webbrowser`:
The `webbrowser` module’s `open(`) function can launch a new browser to a specified URL. For example:
```python
import webbrowser
webbrowser.open('https://inventwithpython.com/')
```
A web browser tab will open to the URL *https://inventwithpython.com*. This is about the only thing the `webbrowser` module can do. Even so, the `open()` function does make some interesting things possible.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For example, it’s tedious to copy a street address to the clipboard every time you’d like to bring up a map of it on OpenStreetMap. You could eliminate a few steps from this task by writing a simple script to automatically launch the map in your browser using the contents of your clipboard. This way, you’d only have to copy the address to a clipboard and run the script for the map to load for you. We can put the address directly into the 
OpenStreetMap URL, so all we need is the `webbrowser.open()` function.  
This is what your program does:
- Gets a street address from the command line arguments or clipboard
- Opens the web browser to the OpenStreetMap page for that address.  

This means your code needs to do the following:
- Read the command line arguments from `sys.argv`.
- Read the clipboard contents.
- Call the `webbrowser.open()` function to open the web browser.
- Open a new file editor tab and save it as *showmap.py*

###  Downloading Files from the Web with the requests Module:
The `requests` module lets you easily download files from the web without having to worry about complicated issues such as network errors, connection routing, and data compression. The module doesn’t come with Python, so you’ll have to install it before you can use it.
#### Downloading Web Pages:
The `requests.get()` function takes a string representing a URL to download. By calling `type()` on the function’s return value, you can see that it returns a `Response` object, which contains the response that the web server gave for your request. Enter the following while your computer is connected to the internet:
```python
import requests

response = requests.get('https://automatetheboringstuff.com/files/rj.txt')
type(response) # <class 'requests.models.Response'>
response.status_code == requests.codes.ok # True
len(response.text) # 178978

print(response.text[:210])

# The Project Gutenberg EBook of Romeo and Juliet, by William Shakespeare
# This eBook is for the use of anyone anywhere at no cost and with  almost no restrictions whatsoever. You may copy it, give it away or
```
The URL takes you to a web page containing the entire text of `Romeo and Juliet`. You can tell that the request for the web page succeeded by checking the `Response` object’s `status_code attribute`. If it’s equal to the value of `requests.codes.ok`, everything went fine. (Incidentally, the status code for “OK” in HTTP is 200. You may already be familiar with the 404 status code for “Not Found.”)  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If the request succeeded, the downloaded web page is stored as a string in the `Response` object’s `text` variable. This large string consists of the entire play; the call to `len(response.text)` shows you that it’s more than 178,000 characters long. Finally, calling `print(response.text[:210])` displays only the first 210 characters.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If the request failed and displayed an error message, like “Failed to establish a new connection” or “Max retries exceeded,” check your internet connection. Connecting to servers can be quite complicated, you can find common causes of your error by doing a web search of the error message in quotes. Also keep in mind that if you download a web page with requests, you’ll get only the HTML content of the web page. You must download images and other media separately.
### Checking for Errors
The `Response` object has a `status_code` attribute that you can check against requests.codes.ok to see whether the download succeeded. A simpler way to check for success is to call the `raise_for_status()` method on the `Response` object. This method will raise an exception if an error occurred while downloading the file and will do nothing if the download succeeded. For example:
```python
response = requests.get('https://inventwithpython.com /page_that_does_not_exist')

response.raise_for_status()
Traceback (most recent call last):
File "<stdin>", line 1, in <module>

File "C:\Users\Al\AppData\Local\Programs\Python\PythonXX\lib\site-packages\requests\models.py", line 940, in raise_for_status
  raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 404 Client Error: Not Found for url: 
https://inventwithpython.com/page_that_does_not_exist.html
```
The `raise_for_status()` method is an easy way to ensure that a program halts if a bad download occurs. Generally, you’ll want your program to stop as soon as some unexpected error happens. If a failed download isn’t a deal breaker, you can wrap the `raise_for_status()` line with try and except statements to handle this error case without crashing:
```python
import requests

response = requests .get('https://inventwithpython .com /page _that _does _not _exist')
try:
  response.raise_for_status()
except Exception as exc:
  print(f'There was a problem: {exc}')
```
This `raise_for_status()` method call causes the program to output the following:
```txt
There was a problem: 404 Client Error: Not Found for url:
https://inventwithpython.com/page_that_does_not_exist.html
```
Always call `raise_for_status()` after calling `requests.get()`. You should be sure that the download has actually worked before your program continues.
### Saving Downloaded Files to the Hard Drive
From here, you can save the web page to a file on your hard drive with the standard `open()` function and `write()` method. However, you must open the file in write binary mode by passing the string `'wb'` as the second argument to `open()`. Even if the page is in plaintext (such as the Romeo and Juliet text you downloaded earlier), you need to write binary data instead of text data in order to maintain the Unicode encoding of the text.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To write the web page to a file, you can use a `for` loop with the `Response` object’s `iter_content()` method:
```python
import requests

response = requests .get('https://automatetheboringstuff .com /files /rj .txt')
response.raise_for_status()
with open('RomeoAndJuliet.txt', 'wb') as play_file:
  for chunk in response.iter_content(100000):
    play_file.write(chunk)
100000
78978
```
The `iter_content()` method returns “chunks” of the content on each iteration through the loop. Each chunk is of the *bytes* data type, and you get to specify how many bytes each chunk will contain. One hundred thousand bytes is generally a good size, so pass `100000` as the argument to `iter_content()`.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The file *RomeoAndJuliet.txt* now exists in the current working directory. Note that while the filename on the website was *rj.txt*, the file on your hard drive has a different filename.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The `write()` method returns the number of bytes written to the file. In the previous example, there were 100,000 bytes in the first chunk, and the remaining part of the file needed only 78,978 bytes.

### A REVIEW OF FILE DOWNLOADING AND SAVING
To review, here’s the complete process for downloading and saving a file:
- Call `requests.get()` to download the file .
- Call `open()` with `'wb'` to create a new file in write binary mode .
- Loop over the Response object’s `iter_content()` method .
- Call `write()` on each iteration to write the content to the file 
